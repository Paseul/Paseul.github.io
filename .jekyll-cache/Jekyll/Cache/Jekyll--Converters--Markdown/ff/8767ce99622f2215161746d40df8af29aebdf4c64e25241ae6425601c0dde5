I"d<h1 id="신경망-아키텍처">신경망 아키텍처</h1>
<h2 id="다층-퍼셉트론multilayer-perceptron-mlp">다층 퍼셉트론(multilayer perceptron, MLP)</h2>
<p>신경망 아키텍처의 가장 기본적인 형태로서 신경 유닛은 층층이 배열되고 인접한 네트워크 층은 전체가 모두 연결된다</p>

<p><img src="https://missinglink.ai/wp-content/uploads/2018/11/multilayer-perceptron.png" alt="출처: https://missinglink.ai/guides/neural-network-concepts/perceptrons-and-multi-layer-perceptrons-the-artificial-neuron-at-the-core-of-deep-learning/" /></p>

<h2 id="오토인코더-신경망autoencoder">오토인코더 신경망(Autoencoder)</h2>
<p>목표값은 입력값과 동일하게 설정된다. 은닉층당 유닛 수가 점진적으로 증가하기 전에 특정 시점까지 점진적으로 감소하고, 최종 층의 차원은 입력 차원과 동일하다. 은닉층의 앞쪽 절반을 인코더(encoder) 뒤쪽 절반을 디코더(decoder)라고 한다.</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png" alt="출처: https://en.wikipedia.org/wiki/Autoencoder" /></p>
<h2 id="변분-오토인코더variational-autoencoders-vaes">변분 오토인코더(variational autoencoders, VAEs)</h2>
<p>디코더 부분은 오토 인코더와 동일하고 인코더 부분에서 확률층(noise)를 추가하여 여러 샘플을 얻어서 최대 가능도로 손실함수를 도출한다</p>

<p><img src="https://miro.medium.com/max/1664/1*eRcdr8gczweQHk--1pZF9A@2x.png" alt="출처: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" /></p>

<h2 id="적대적-신경망generatice-adversarial-networks-gan">적대적 신경망(Generatice Adversarial Networks, GAN)</h2>
<p>GAN에는 생성기 신경망과 판별기 신경망이 있다. 생성기 네트워크는 임의의 소음을 입력받아 데이터 샘플을 생성하려고 시도한다. 판별기 네트워크는 생성된 데이터를 실제 데이터와 비교하고 생성된 데이터가 가짜인지 아닌지에 대한 이진 분류 문제를 시그모이드 출력 활성화를 사용해 해결한다.</p>

<p><img src="https://developers.google.com/machine-learning/gan/images/gan_diagram_discriminator.svg" alt="출처: https://developers.google.com/machine-learning/gan/discriminator" /></p>

<h2 id="lenet">LeNet</h2>
<p>1998년에 설계한 7단계 합성곱 네트워크로, 숫자 분류에 사용된다.</p>

<p><img src="https://notebooks.azure.com/rodolfoferro/libraries/KerasMNIST/raw/LeNet-5.png" alt="출처: https://notebooks.azure.com/rodolfoferro/projects/KerasMNIST/html/MNIST%20-%20CNN%20Model.ipynb" /></p>

<h2 id="alexnet">AlexNet</h2>
<p>2012년 ILSVRC우승팀의 아키텍처. LeNet과 매우 유사한 아키텍처를 가지고 있지만, 층당 필터가 더 많고 깊다. 또한 항상 대체된 합성곱 풀링 대신 스택 합성곱을 사용한다. 작은 합성곱의 스택은 합성곱층의 하나로 된 커다란 수용 영역보다 낫다. 더 많은 비선형성과 더 적은 파라미터를 도입하기 때문이다.</p>

<p><img src="http://openresearch.ai/uploads/default/original/1X/b25dce7cb47b0a80b3631d10476a630df4b1f2ff.jpg" alt="출처: Alexnet 논문" /></p>
<blockquote>
  <p>출처: 딥러닝 전이학습, “위키북스”</p>
</blockquote>
:ET